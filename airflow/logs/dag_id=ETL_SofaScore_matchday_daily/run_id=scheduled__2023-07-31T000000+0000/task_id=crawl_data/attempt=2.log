[2024-07-27T13:25:41.729+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-27T13:25:41.760+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T13:25:41.771+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T13:25:41.784+0000] {taskinstance.py:2306} INFO - Starting attempt 2 of 3
[2024-07-27T13:25:41.964+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-07-31 00:00:00+00:00
[2024-07-27T13:25:41.971+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=145) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-07-27T13:25:41.973+0000] {standard_task_runner.py:63} INFO - Started process 149 to run task
[2024-07-27T13:25:41.972+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'ETL_SofaScore_Matchday_daily', 'crawl_data', 'scheduled__2023-07-31T00:00:00+00:00', '--job-id', '277', '--raw', '--subdir', 'DAGS_FOLDER/crawl_dags.py', '--cfg-path', '/tmp/tmpvuftche_']
[2024-07-27T13:25:41.975+0000] {standard_task_runner.py:91} INFO - Job 277: Subtask crawl_data
[2024-07-27T13:25:42.025+0000] {task_command.py:426} INFO - Running <TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [running]> on host fde3e54b98e9
[2024-07-27T13:25:42.111+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='nguyenmduc2407@gmail.com' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_SofaScore_Matchday_daily' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-31T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-31T00:00:00+00:00'
[2024-07-27T13:25:42.113+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-27T13:25:44.278+0000] {logging_mixin.py:188} INFO - Dataframe is here!!!
[2024-07-27T13:25:44.347+0000] {logging_mixin.py:188} INFO -       customId  winnerCode  ...  awayScore_extra2 awayScore_overtime
0      kYcsmYc         2.0  ...               NaN                NaN
1      vYcsDPi         3.0  ...               NaN                NaN
2    Didbsbxpc         2.0  ...               NaN                NaN
3      tYcswXo         1.0  ...               NaN                NaN
4      nYcspYc         2.0  ...               NaN                NaN
..         ...         ...  ...               ...                ...
234    GoksUjn         1.0  ...               NaN                NaN
235     EPsTkk         3.0  ...               NaN                NaN
236    ZTqsoGB         2.0  ...               NaN                NaN
237     UHsXdb         2.0  ...               NaN                NaN
238   KrcsBHtb         3.0  ...               NaN                NaN

[239 rows x 129 columns]
[2024-07-27T13:25:44.675+0000] {logging_mixin.py:188} INFO - Data has been written to the database.
[2024-07-27T13:25:44.680+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-07-27T13:25:44.681+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-27T13:25:44.689+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ETL_SofaScore_Matchday_daily, task_id=crawl_data, run_id=scheduled__2023-07-31T00:00:00+00:00, execution_date=20230731T000000, start_date=20240727T132541, end_date=20240727T132544
[2024-07-27T13:25:44.720+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-07-27T13:25:44.747+0000] {taskinstance.py:3498} INFO - 7 downstream tasks scheduled from follow-on schedule check
[2024-07-27T13:25:44.751+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-07-27T15:09:06.264+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-27T15:09:06.298+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T15:09:06.311+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T15:09:06.311+0000] {taskinstance.py:2306} INFO - Starting attempt 2 of 3
[2024-07-27T15:09:07.065+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-07-31 00:00:00+00:00
[2024-07-27T15:09:07.075+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=1585) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-07-27T15:09:07.077+0000] {standard_task_runner.py:63} INFO - Started process 1587 to run task
[2024-07-27T15:09:07.076+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'ETL_SofaScore_Matchday_daily', 'crawl_data', 'scheduled__2023-07-31T00:00:00+00:00', '--job-id', '319', '--raw', '--subdir', 'DAGS_FOLDER/crawl_dags.py', '--cfg-path', '/tmp/tmpx7qpw984']
[2024-07-27T15:09:07.079+0000] {standard_task_runner.py:91} INFO - Job 319: Subtask crawl_data
[2024-07-27T15:09:07.131+0000] {task_command.py:426} INFO - Running <TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [running]> on host fde3e54b98e9
[2024-07-27T15:09:07.221+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='nguyenmduc2407@gmail.com' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_SofaScore_Matchday_daily' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-31T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-31T00:00:00+00:00'
[2024-07-27T15:09:07.223+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-27T15:09:09.181+0000] {logging_mixin.py:188} INFO - Dataframe is here!!!
[2024-07-27T15:09:09.265+0000] {logging_mixin.py:188} INFO -       customId  winnerCode  ...  awayScore_extra2 awayScore_overtime
0      kYcsmYc         2.0  ...               NaN                NaN
1      vYcsDPi         3.0  ...               NaN                NaN
2    Didbsbxpc         2.0  ...               NaN                NaN
3      tYcswXo         1.0  ...               NaN                NaN
4      nYcspYc         2.0  ...               NaN                NaN
..         ...         ...  ...               ...                ...
234    GoksUjn         1.0  ...               NaN                NaN
235     EPsTkk         3.0  ...               NaN                NaN
236    ZTqsoGB         2.0  ...               NaN                NaN
237     UHsXdb         2.0  ...               NaN                NaN
238   KrcsBHtb         3.0  ...               NaN                NaN

[239 rows x 129 columns]
[2024-07-27T15:09:09.661+0000] {logging_mixin.py:188} INFO - Data has been written to the database.
[2024-07-27T15:09:09.676+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-07-27T15:09:09.683+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-27T15:09:09.703+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ETL_SofaScore_Matchday_daily, task_id=crawl_data, run_id=scheduled__2023-07-31T00:00:00+00:00, execution_date=20230731T000000, start_date=20240727T150906, end_date=20240727T150909
[2024-07-27T15:09:09.784+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-07-27T15:09:09.818+0000] {taskinstance.py:3498} INFO - 7 downstream tasks scheduled from follow-on schedule check
[2024-07-27T15:09:09.822+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-07-27T15:54:56.845+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-27T15:54:56.884+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T15:54:56.894+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T15:54:56.895+0000] {taskinstance.py:2306} INFO - Starting attempt 2 of 3
[2024-07-27T15:54:57.055+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-07-31 00:00:00+00:00
[2024-07-27T15:54:57.063+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=2277) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-07-27T15:54:57.064+0000] {standard_task_runner.py:63} INFO - Started process 2279 to run task
[2024-07-27T15:54:57.064+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'ETL_SofaScore_Matchday_daily', 'crawl_data', 'scheduled__2023-07-31T00:00:00+00:00', '--job-id', '355', '--raw', '--subdir', 'DAGS_FOLDER/crawl_dags.py', '--cfg-path', '/tmp/tmpjlkhxp5m']
[2024-07-27T15:54:57.066+0000] {standard_task_runner.py:91} INFO - Job 355: Subtask crawl_data
[2024-07-27T15:54:57.115+0000] {task_command.py:426} INFO - Running <TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [running]> on host fde3e54b98e9
[2024-07-27T15:54:57.203+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='nguyenmduc2407@gmail.com' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_SofaScore_Matchday_daily' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-31T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-31T00:00:00+00:00'
[2024-07-27T15:54:57.204+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-27T15:54:59.334+0000] {logging_mixin.py:188} INFO - Dataframe is here!!!
[2024-07-27T15:54:59.427+0000] {logging_mixin.py:188} INFO -       customId  winnerCode  ...  awayScore_extra2 awayScore_overtime
0      kYcsmYc         2.0  ...               NaN                NaN
1      vYcsDPi         3.0  ...               NaN                NaN
2    Didbsbxpc         2.0  ...               NaN                NaN
3      tYcswXo         1.0  ...               NaN                NaN
4      nYcspYc         2.0  ...               NaN                NaN
..         ...         ...  ...               ...                ...
234    GoksUjn         1.0  ...               NaN                NaN
235     EPsTkk         3.0  ...               NaN                NaN
236    ZTqsoGB         2.0  ...               NaN                NaN
237     UHsXdb         2.0  ...               NaN                NaN
238   KrcsBHtb         3.0  ...               NaN                NaN

[239 rows x 129 columns]
[2024-07-27T15:54:59.893+0000] {logging_mixin.py:188} INFO - Data has been written to the database.
[2024-07-27T15:54:59.899+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-07-27T15:54:59.901+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-27T15:54:59.915+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ETL_SofaScore_Matchday_daily, task_id=crawl_data, run_id=scheduled__2023-07-31T00:00:00+00:00, execution_date=20230731T000000, start_date=20240727T155456, end_date=20240727T155459
[2024-07-27T15:54:59.971+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-07-27T15:55:00.006+0000] {taskinstance.py:3498} INFO - 7 downstream tasks scheduled from follow-on schedule check
[2024-07-27T15:55:00.012+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-07-27T16:27:24.993+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-27T16:27:25.025+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T16:27:25.037+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T16:27:25.038+0000] {taskinstance.py:2306} INFO - Starting attempt 2 of 3
[2024-07-27T16:27:25.242+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-07-31 00:00:00+00:00
[2024-07-27T16:27:25.249+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=2731) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-07-27T16:27:25.252+0000] {standard_task_runner.py:63} INFO - Started process 2733 to run task
[2024-07-27T16:27:25.251+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'ETL_SofaScore_Matchday_daily', 'crawl_data', 'scheduled__2023-07-31T00:00:00+00:00', '--job-id', '368', '--raw', '--subdir', 'DAGS_FOLDER/crawl_dags.py', '--cfg-path', '/tmp/tmp7006cxmo']
[2024-07-27T16:27:25.253+0000] {standard_task_runner.py:91} INFO - Job 368: Subtask crawl_data
[2024-07-27T16:27:25.299+0000] {task_command.py:426} INFO - Running <TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [running]> on host fde3e54b98e9
[2024-07-27T16:27:25.380+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='nguyenmduc2407@gmail.com' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_SofaScore_Matchday_daily' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-31T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-31T00:00:00+00:00'
[2024-07-27T16:27:25.382+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-27T16:27:26.936+0000] {logging_mixin.py:188} INFO - Dataframe is here!!!
[2024-07-27T16:27:27.170+0000] {logging_mixin.py:188} INFO -       customId  winnerCode  ...  awayScore_extra2 awayScore_overtime
0      kYcsmYc         2.0  ...               NaN                NaN
1      vYcsDPi         3.0  ...               NaN                NaN
2    Didbsbxpc         2.0  ...               NaN                NaN
3      tYcswXo         1.0  ...               NaN                NaN
4      nYcspYc         2.0  ...               NaN                NaN
..         ...         ...  ...               ...                ...
234    GoksUjn         1.0  ...               NaN                NaN
235     EPsTkk         3.0  ...               NaN                NaN
236    ZTqsoGB         2.0  ...               NaN                NaN
237     UHsXdb         2.0  ...               NaN                NaN
238   KrcsBHtb         3.0  ...               NaN                NaN

[239 rows x 129 columns]
[2024-07-27T16:27:27.468+0000] {logging_mixin.py:188} INFO - Data has been written to the database.
[2024-07-27T16:27:27.474+0000] {python.py:237} INFO - Done. Returned value was: (['10834018', '10834019', '10834021', '10834020', '10834022', '10834023', '10834024', '10834025', '11067597', '11067611', '11067595', '11067601', '11369787', '11369789', '11369790', '10981560', '10981558', '10981625', '10981569', '10981584', '11347285', '11347286', '11471485', '11471490', '11471489', '11471492', '11076542', '11076538', '11076534', '11423813', '11423815', '11423812', '11423814', '11423817', '11423819', '11423816', '11423818', '11156282', '11156265', '11156267', '11156304', '11156301', '11156291', '11156298', '11156296', '11156286', '11156287', '11387604', '10961718', '10961715', '10961719', '10961720', '11434969', '11434971', '11434973', '11434975', '11434977', '11434965', '11434979', '11367610', '11367607', '11367611', '11367613', '11351393', '11351390', '11351391', '11351387', '11351389', '11369784', '11369786', '11369788', '11370322', '11200807', '11200803', '11200818', '11200806', '10932748', '10932743', '10932753', '10932749', '10932740', '10932758', '11464258', '11464257', '11464262', '11464261', '11384922', '11384940', '11384921', '11384919', '10923242', '10923266', '10923249', '10923263', '10923254', '11367837', '11367835', '11367834', '11034975', '10986770', '11378629', '11378630', '11378628', '11378626', '11377787', '11029820', '11029819', '11375097', '11375164', '11388311', '11388314', '10997551', '10997543', '10997550', '11458777', '11000166', '11000174', '11000173', '11000180', '11000170', '11000172', '11000179', '11471480', '11404252', '11404246', '11404241', '11114400', '11114404', '11114406', '11114409', '11114408', '11114411', '11351004', '11351002', '11351001', '10989080', '10989084', '10989086', '10989079', '11415080', '11415076', '11465714', '11354445', '11354443', '11354444', '11367062', '11364729', '11378895', '11481590', '11015995', '11015996', '11015990', '11015989', '11030095', '11030093', '11030094', '11434943', '11434942', '11434944', '11434941', '11434936', '11434935', '11432987', '11446176', '10976487', '10976489', '10975798', '10976493', '11383010', '11383007', '11035359', '11035431', '11035403', '11070054', '11070040', '11379716', '11328634', '11328635', '11021132', '11434648', '11434650', '11432994', '11434645', '10950758', '10950763', '10950776', '10950779', '11469112', '11472152', '11367372', '11367364', '11367319', '11367368', '11470243', '11470245', '11443534', '11443536', '11443537', '11443533', '11443531', '11443532', '11443541', '11349094', '11041316', '11374601', '11374599', '11374600', '10926583', '10926580', '10926592', '11367702', '11367704', '11408816', '11408802', '11408809', '11472372', '11404818', '11487500', '11487497', '11395860', '11418178', '11418180', '11487503', '11447611', '11273191', '11487514', '11336965', '11476404', '11487518', '11487516', '11273192', '11396983', '11473199', '11488872', '11490580', '11490594', '11489295', '11331891', '11457040'], 0        kYcsmYc
1        vYcsDPi
2      Didbsbxpc
3        tYcswXo
4        nYcspYc
         ...    
234      GoksUjn
235       EPsTkk
236      ZTqsoGB
237       UHsXdb
238     KrcsBHtb
Name: customId, Length: 239, dtype: object)
[2024-07-27T16:27:27.477+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-27T16:27:27.487+0000] {xcom.py:675} ERROR - Object of type tuple is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your *** config or make sure to decorate your object with attr.
[2024-07-27T16:27:27.490+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 91, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 150, in serialize
    return encode(classname or serialized_classname, version, serialize(data, depth + 1))
                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 127, in serialize
    return [serialize(d, depth + 1) for d in o]
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 189, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'pandas.core.series.Series'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 486, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3197, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 246, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 673, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 102, in encode
    o = self.default(o)
        ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 93, in default
    return super().default(o)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type tuple is not JSON serializable
[2024-07-27T16:27:27.503+0000] {taskinstance.py:1206} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_SofaScore_Matchday_daily, task_id=crawl_data, run_id=scheduled__2023-07-31T00:00:00+00:00, execution_date=20230731T000000, start_date=20240727T162725, end_date=20240727T162727
[2024-07-27T16:27:27.524+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/utils/email.py:154: RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
  send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)

[2024-07-27T16:27:27.525+0000] {configuration.py:1050} WARNING - section/key [smtp/smtp_user] not found in config
[2024-07-27T16:27:27.526+0000] {email.py:271} INFO - Email alerting: attempt 1
[2024-07-27T16:27:27.537+0000] {configuration.py:1050} WARNING - section/key [smtp/smtp_user] not found in config
[2024-07-27T16:27:27.538+0000] {email.py:271} INFO - Email alerting: attempt 1
[2024-07-27T16:27:27.539+0000] {taskinstance.py:879} ERROR - Failed to send email to: nguyenmduc2407@gmail.com
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 91, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 150, in serialize
    return encode(classname or serialized_classname, version, serialize(data, depth + 1))
                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 127, in serialize
    return [serialize(d, depth + 1) for d in o]
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 189, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'pandas.core.series.Series'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2676, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2701, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 486, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3197, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 246, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 673, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 102, in encode
    o = self.default(o)
        ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 93, in default
    return super().default(o)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type tuple is not JSON serializable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/socket.py", line 852, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.12/socket.py", line 837, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/socket.py", line 852, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.12/socket.py", line 837, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
[2024-07-27T16:27:27.574+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 368 for task crawl_data (Object of type tuple is not JSON serializable; 2733)
[2024-07-27T16:27:27.595+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-07-27T16:27:27.647+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-27T16:27:27.652+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-07-27T17:26:01.254+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-27T17:26:01.282+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T17:26:01.292+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [queued]>
[2024-07-27T17:26:01.293+0000] {taskinstance.py:2306} INFO - Starting attempt 2 of 3
[2024-07-27T17:26:01.488+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-07-31 00:00:00+00:00
[2024-07-27T17:26:01.494+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=3626) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-07-27T17:26:01.496+0000] {standard_task_runner.py:63} INFO - Started process 3628 to run task
[2024-07-27T17:26:01.496+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'ETL_SofaScore_Matchday_daily', 'crawl_data', 'scheduled__2023-07-31T00:00:00+00:00', '--job-id', '414', '--raw', '--subdir', 'DAGS_FOLDER/crawl_dags.py', '--cfg-path', '/tmp/tmpih8xmw8e']
[2024-07-27T17:26:01.498+0000] {standard_task_runner.py:91} INFO - Job 414: Subtask crawl_data
[2024-07-27T17:26:01.553+0000] {task_command.py:426} INFO - Running <TaskInstance: ETL_SofaScore_Matchday_daily.crawl_data scheduled__2023-07-31T00:00:00+00:00 [running]> on host fde3e54b98e9
[2024-07-27T17:26:01.644+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='nguyenmduc2407@gmail.com' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ETL_SofaScore_Matchday_daily' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-31T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-31T00:00:00+00:00'
[2024-07-27T17:26:01.646+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-27T17:26:03.633+0000] {logging_mixin.py:188} INFO - Dataframe is here!!!
[2024-07-27T17:26:03.744+0000] {logging_mixin.py:188} INFO -       customId  winnerCode  ...  awayScore_extra2 awayScore_overtime
0      kYcsmYc         2.0  ...               NaN                NaN
1      vYcsDPi         3.0  ...               NaN                NaN
2    Didbsbxpc         2.0  ...               NaN                NaN
3      tYcswXo         1.0  ...               NaN                NaN
4      nYcspYc         2.0  ...               NaN                NaN
..         ...         ...  ...               ...                ...
234    GoksUjn         1.0  ...               NaN                NaN
235     EPsTkk         3.0  ...               NaN                NaN
236    ZTqsoGB         2.0  ...               NaN                NaN
237     UHsXdb         2.0  ...               NaN                NaN
238   KrcsBHtb         3.0  ...               NaN                NaN

[239 rows x 129 columns]
[2024-07-27T17:26:04.151+0000] {logging_mixin.py:188} INFO - Data has been written to the database.
[2024-07-27T17:26:04.156+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-07-27T17:26:04.158+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-27T17:26:04.172+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=ETL_SofaScore_Matchday_daily, task_id=crawl_data, run_id=scheduled__2023-07-31T00:00:00+00:00, execution_date=20230731T000000, start_date=20240727T172601, end_date=20240727T172604
[2024-07-27T17:26:04.240+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-07-27T17:26:04.277+0000] {taskinstance.py:3498} INFO - 7 downstream tasks scheduled from follow-on schedule check
[2024-07-27T17:26:04.282+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
